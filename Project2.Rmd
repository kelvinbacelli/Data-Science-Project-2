---
author: "Matthew Arsenault and Kelvin Bacelli"
title: "Effects on Cost of Health Insurance"
date: "2025-12-12"
output:
  pdf_document:
    number_sections: true
editor_options: 
  chunk_output_type: inline
---
```{r echo=FALSE, include=FALSE, message=FALSE}
# all library load statements here
library(caret)
library(knitr)
library(xgboost)
library(rattle)
r.version <- "4.3.2"
seed.val <- 123456
```

With many different financial concerns people need to worry about, health insurance is a major necessary expense that most consider worth having. With different groups of people being in different situations, what characteristics can best explain how much an individual will have to pay for health insurance best? In this analysis, we used a linear regression model and a random forest tree model to analyse data from expense information to better understand the effects of different characteristics on the charges of health insurance.

# Data Description and Processing

This data set (1) is frequently taken from publicly accessible health insurance expense information on individuals with health insurance.

This data set contains descriptions of the individuals as well as how much they are charged for their insurance.\
Outcomes: \
charges: The medical insurance cost that the individual get charged. \
Descriptors:\
age: The age of the individual.\
sex: The sex of the individual.\
bmi: The body mass index of the individual that measures if they are at a health weight when considering height.\
children: The amount of children that the individual has.\
smoker: Either true or false of if the individual is a smoker.\
region: The area that the individual is located in.

This data set was checked for missing values. Of the original 1338 rows, only 3 of them contained missing values. After the removal of those rows, we are left with 1335 for examination.

```{r echo=FALSE, include=FALSE}
data.df <- read.csv("insurance.csv")
data.df <- na.omit(data.df)
data.df$smoker <- factor(data.df$smoker)
data.df$region <- factor(data.df$region)
data.df$sex <- factor(data.df$sex)
```


# Data Exploration

This dataset contains observations on health insurance costs for smokers as well as non-smokers. Since smoking is likely to influence insurance costs, we compare the number of smokers vs non-smokers. The distribution shows that most people are non-smokers(~80%).

\newpage

```{r echo=FALSE}
label = c("non-smoker","smoker")
count = c("1062","273")
owner.tbl <- cbind(label,count)
kable(owner.tbl, caption = "Smoker Comparison")
```

We also explored the distribution of our outcome variable, charges, to understand the overall spread of health insurance costs in the dataset. The distribution is heavily right-skewed, indicating that while most people pay relatively low premiums, a small number of people pay very high premiums.


```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Selling Price (US dollars)."}
hist(data.df$charges, freq = FALSE, main = "", xlab = "Value",ylab = "Density")

lines(density(data.df$charges),col = "red",lwd=2)
```



```{r echo=FALSE}
# table of descriptive stats for the overall life expectancy
Statistics <- c("Min", "Mean","Median","StdDev","Max")
mn<-min(data.df$charges)
mx<-max(data.df$charges)
avg<-round(mean(data.df$charges), 2)
md<-median(data.df$charges)
std<-round(sd(data.df$charges), 2)
Value <- c(mn,avg,md,std,mx)
st.tbl <- cbind(Statistics, Value)
kable(st.tbl, caption="Overall Premiums Statistics")
```


Due to the fact that the original charges variable was highly right-skewed, we applied the natural logarithm to the graph in order to make the distribution more symmetric. This helps stabilize the variance and makes the data more suitable for statistical modeling techniques (which assume approximately normal residuals). Because the natural log scale is exponential, each one-unit increase in log(selling price) corresponds to roughly a 2.7x increase in the actual selling price, meaning values from 10 to 11 represent about a 2.7 times increase, and from 11 to 12 another 2.7 times increase.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Selling Price (US dollars)."}

hist(log(data.df$charges), freq = FALSE, main = "", xlab = "Log(charges)",ylab = "Density")

lines(density(log(data.df$charges)),col = "red",lwd=2)
```


We created a boxplot of the log-transformed charges across the two categories to explore how smoking affects health insurance costs. As shown in the plot, if a person smokes, the mean insurance cost is significantly higher, indicating that people who smoke pay much higher premiums than those who don't.

```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Selling Price by Ownership Level."}
#data.df$smoker <- trimws(data.df$smoker)  # remove any stray spaces

#data.df$smoker <- factor(data.df$smoker,levels = c("no", "yes"))

par(mar = c(8,4,4,2))
boxplot(log(charges)~smoker,data = data.df,main="",ylab = "Log(charges)", xlab = "Smoker Category", las=1,cex.axis=0.8)

```


```{r echo=FALSE, include=FALSE}
first.df  <- subset(data.df, smoker == "no")
second.df <- subset(data.df, smoker == "yes")
```

```{r echo=FALSE}
# table of descriptive stats for the overall life expectancy
statistics <- c("Min", "Mean","Median","StdDev","Max")
#developed
data1<-first.df$charges
mn1<-min(data1)
mx1<-max(data1)
avg1<-round(mean(data1), 2)
md1<-median(data1)
std1<-round(sd(data1), 2)
first <- c(mn1,avg1,md1,std1,mx1)
#developing
data2<-second.df$charges
mn2<-min(data2)
mx2<-max(data2)
avg2<-round(mean(data2), 2)
md2<-median(data2)
std2<-round(sd(data2), 2)
second <- c(mn2,avg2,md2,std2,mx2)


tble.df<- data.frame(statistics, nonsmokers = first, smokers = second)
kable(tble.df, caption="Premiums for smokers vs non-smokers")

```


A t-test was conducted to test whether mean log(charges) differed across smoking categories. The results were highly significant (p-value < 0.05), meaning that the difference between the means of the two groups is unlikely to be due to chance.

```{r echo=FALSE, include=FALSE} 
t.test(log(charges)~smoker, data = data.df)
```

# Modeling the effect of Characteristics on Insurance Costs

What will the impacts on insurance charges be based of an individuals characteristics?
Our dataset contains descriptive information about each person as well as the total cost they are charged for their health insurance. The variables are both personal characteristics and personal choices that may influence certain health risks. Some variables capture biological factors such as age or an individual's body mass index, while others describe behaviors or circumstances, like whether the individual smokes or the number of children that an individual has, that could potentially increase or decrease expected medical costs. Understanding how each factor relates to charges can help show how much lower or higher an individual's insurance cost could be if certain characteristics were different.

## Hypothesis

We hypothesize that having a higher BMI, children count, age, and being a smoker will heighten the charges that an individual will have to pay the most, as there are a higher amount of risks that need to be accounted for.

## Model 1:

We fit a linear regression step-wise model using all of the characteristics as predictor variables, those being age, sex, bmi, children, smoker, and region, and the insurance cost as the dependent variable. After putting our original model into the step wise function, we were able to remove the sex predictor to increase the simplicity of our model without taking away how much variation it explains. The fits results are displayed below

```{r echo = FALSE}
fit1 <- lm(charges ~ age + sex + bmi + children + smoker + region, data = data.df)
fit.step <- step(fit1, trace = 0)
coef.vals <- coef(summary(fit.step))
kable(coef.vals)
```

\newpage

We can see that of the predictor variables, age, bmi, children, and yes to being a smoker are all highly significant, with different regions being slightly significant but not by much. With all of our predictors, the adjusted R squared value came to 0.7506, meaning that just over 75% of the variation in the data can be explained by the predictors included in our model. 

We can use a histogram and a QQ plot to show the distribution of our residuals, which measure our fit's errors.
```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="Distribution of Residuals: Model 1."}
par(mfrow=c(1,2))
hist(fit.step$residuals, freq = FALSE, main = "", xlab = "Residuals") 
lines(density(fit.step$residuals))

qqnorm(y=fit.step$residuals, main = "")
qqline(y=fit.step$residuals, datax = FALSE)
```

We can see a slightly normal distribution of residuals around 0, with our density curve showing some weight to the left and right of 0. We can also see that the histogram has a heavy tail to the right of our center mass. This same pattern is able to be seen in the QQ plot, as to the left and right of 0, our residuals stray off of the line of normality, with a tail straying far off on the far right.

## Model 2: 

For our second model, we fit a boosted tree which works by building many small decision trees, where each tree attempts to correct the errors of the previous ones. This allows the model to capture non-linear patterns in the data that a linear model cannot.

We trained the boosted tree using all predictor variables and printed out the data summary for the best model. As depicted, the R squared value came out to be ~0.88 meaning that this model can explain a much larger percentage (88%) of the data than the linear model.
```{r echo = false}
fit2 <- train(charges~.,data = data.df,method="xgbTree",verbosity=0)
pred2 <- predict(fit2, newdata = data.df)
postResample(pred2, obs = data.df$charges)
```

Here, we examined which predictor variables contributed most to the performance of the boosted tree model. We can see that the variables smoker, BMI, and age are the most important factors attributed to health insurance cost and that other factors like sex and certain regions are not very important. Overall, this aligns with our findings in model 1. 


```{r echo = false}
fit2.imp <- varImp(fit2)
plot(fit2.imp)
```

We can analyze the errors made by the boosted tree model by examining the residuals between the predicted and actual charges.

Unlike the residuals of model 1 (which were approximately normally distributed), the residuals of the boosted tree model are not normally distributed. However, this is not a problem as boosted trees do not assume normality of residuals so this is an expected and acceptable outcome.

```{r echo = false}
pred <- predict(fit2, newdata = data.df)

# Residuals
res <- data.df$charges - pred

# Histogram
hist(res, main="Residual Distribution", xlab="Residuals")

# QQ plot
qqnorm(res)
qqline(res)
```


Overall, the boosted tree model provided a noticeably stronger fit than the stepwise linear regression model (as indicated by the much higher R squared value). Despite this, the general trends identified by model 1 remain consistent and show that factors like smoker status, BMI, and age are the most influential predictors of health insurance cost.

# Predicting with Models 1 and 2

Now that we have two distinct models, we can work with both of them to see which one is a better predictor of health insurance cost given data. We can use a 10-fold cross validation to do this, separating 80% of the data to use for training, and the other 20% for testing on each fold.

```{r echo=FALSE}
train.control <- trainControl(method = "cv", number = 10, p = 0.80) 
RNGversion(r.version)
set.seed(seed.val)

fit1.train <- train(charges ~ age + sex + bmi + children + smoker + region, data = data.df, method = "lm", trControl = train.control)

fit2.train <- train(charges~.,data = data.df,method="xgbTree",verbosity=0,trControl=train.control)

```


```{r echo=FALSE, out.width="60%", out.height="40%", fig.align = 'center', fig.cap="RMSE Over 10 Folds."}
num.folds <- 10
rmse.fit1 <- fit1.train$resample$RMSE
rmse.fit2 <- fit2.train$resample$RMSE

plot(
  rmse.fit1, type = "b",
  xlab = "Fold", ylab = "RMSE",
  col = "blue",
  ylim = c(min(c(rmse.fit1, rmse.fit2)),
           max(c(rmse.fit1, rmse.fit2)))
)

lines(rmse.fit2, type = "b", col = "red", lty = 2)
legend("topright", legend=c("Model 1 (Linear)", "Model 2 (Boosted Tree)"),
       col=c("blue", "red"), lty=c(1,2))
```


We can now calculate the average root mean squared error (RMSE) for both of our models' predictions of the 10 folds. The results are below.

```{r echo=FALSE}
label <- c("Model 1", "Model 2")
avg.fit1 <- round(mean(rmse.fit1), 2)
avg.fit2 <- round(mean(rmse.fit2), 2)
rmse <- c(avg.fit1, avg.fit2)

results.tbl <- cbind(label, rmse)
kable(results.tbl, caption="Mean RMSE for Models 1 and 2.")
```

# Summary and Conclusions


# Refrences 

1: https://www.kaggle.com/datasets/nalisha/health-insurance-charges-dataset
